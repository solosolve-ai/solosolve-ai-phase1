{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c4ed5fcc",
      "metadata": {
        "id": "c4ed5fcc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ec697d76",
      "metadata": {
        "id": "ec697d76"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "seq_len = 20\n",
        "dropout = 0.5\n",
        "num_epochs = 10\n",
        "label_col = \"Product\"\n",
        "tokens_path = \"Output/tokens.pkl\"\n",
        "labels_path = \"Output/labels.pkl\"\n",
        "data_path = \"Input/complaints.csv\"\n",
        "model_path = \"Output/bert_pre_trained.pth\"\n",
        "text_col_name = \"Consumer complaint narrative\"\n",
        "label_encoder_path = \"Output/label_encoder.pkl\"\n",
        "product_map = {'Vehicle loan or lease': 'vehicle_loan',\n",
        "               'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n",
        "               'Credit card or prepaid card': 'card',\n",
        "               'Money transfer, virtual currency, or money service': 'money_transfer',\n",
        "               'virtual currency': 'money_transfer',\n",
        "               'Mortgage': 'mortgage',\n",
        "               'Payday loan, title loan, or personal loan': 'loan',\n",
        "               'Debt collection': 'debt_collection',\n",
        "               'Checking or savings account': 'savings_account',\n",
        "               'Credit card': 'card',\n",
        "               'Bank account or service': 'savings_account',\n",
        "               'Credit reporting': 'credit_report',\n",
        "               'Prepaid card': 'card',\n",
        "               'Payday loan': 'loan',\n",
        "               'Other financial service': 'others',\n",
        "               'Virtual currency': 'money_transfer',\n",
        "               'Student loan': 'loan',\n",
        "               'Consumer Loan': 'loan',\n",
        "               'Money transfers': 'money_transfer'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4da2b9ec",
      "metadata": {
        "id": "4da2b9ec"
      },
      "outputs": [],
      "source": [
        "def save_file(name, obj):\n",
        "    \"\"\"\n",
        "    Function to save an object as pickle file\n",
        "    \"\"\"\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def load_file(name):\n",
        "    \"\"\"\n",
        "    Function to load a pickle object\n",
        "    \"\"\"\n",
        "    return pickle.load(open(name, \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64246157",
      "metadata": {
        "id": "64246157"
      },
      "source": [
        "## Process text data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ccad4958",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ccad4958",
        "outputId": "325a42e4-e52f-4a74-89db-ee095d113832"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 59132",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-771f8fc37c08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered_DATASET.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 59132"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('filtered_DATASET.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "45e6a0e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "45e6a0e5",
        "outputId": "1c6d5b84-251b-47e6-f9ca-83494f9fd9ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5b8ef1cff98d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.dropna(subset=[text_col_name], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e14a0c5",
      "metadata": {
        "id": "0e14a0c5"
      },
      "outputs": [],
      "source": [
        "data.replace({label_col: product_map}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7207cee2",
      "metadata": {
        "id": "7207cee2"
      },
      "source": [
        "### Encode labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49752a60",
      "metadata": {
        "id": "49752a60"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(data[label_col])\n",
        "labels = label_encoder.transform(data[label_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b912ad",
      "metadata": {
        "id": "e7b912ad"
      },
      "outputs": [],
      "source": [
        "save_file(labels_path, labels)\n",
        "save_file(label_encoder_path, label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d53e405c",
      "metadata": {
        "id": "d53e405c"
      },
      "source": [
        "### Process the text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ffaab7",
      "metadata": {
        "id": "34ffaab7"
      },
      "outputs": [],
      "source": [
        "input_text = list(data[text_col_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14034928",
      "metadata": {
        "id": "14034928"
      },
      "outputs": [],
      "source": [
        "len(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a867d36",
      "metadata": {
        "id": "4a867d36"
      },
      "source": [
        "### Convert text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad2a95f",
      "metadata": {
        "id": "5ad2a95f"
      },
      "outputs": [],
      "source": [
        "input_text = [i.lower() for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cd359f",
      "metadata": {
        "id": "51cd359f"
      },
      "source": [
        "### Remove punctuations except apostrophe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1cde62",
      "metadata": {
        "id": "2e1cde62"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i)\n",
        "             for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f4a1f5",
      "metadata": {
        "id": "b9f4a1f5"
      },
      "source": [
        "### Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d610a9a8",
      "metadata": {
        "id": "d610a9a8"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6157ab6e",
      "metadata": {
        "id": "6157ab6e"
      },
      "source": [
        "### Remove more than one consecutive instance of 'x'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e765d86",
      "metadata": {
        "id": "8e765d86"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e74b80fc",
      "metadata": {
        "id": "e74b80fc"
      },
      "source": [
        "### Remove multiple spaces with single space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c399a3",
      "metadata": {
        "id": "68c399a3"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46fa09e2",
      "metadata": {
        "id": "46fa09e2"
      },
      "source": [
        "### Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5232d10a",
      "metadata": {
        "id": "5232d10a"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c606d801",
      "metadata": {
        "id": "c606d801"
      },
      "outputs": [],
      "source": [
        "input_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ba20a4",
      "metadata": {
        "id": "51ba20a4"
      },
      "outputs": [],
      "source": [
        "sample_tokens = tokenizer(input_text[0], padding=\"max_length\",\n",
        "                         max_length=seq_len, truncation=True,\n",
        "                         return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03334517",
      "metadata": {
        "id": "03334517"
      },
      "outputs": [],
      "source": [
        "sample_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7a9ba6",
      "metadata": {
        "id": "5d7a9ba6"
      },
      "outputs": [],
      "source": [
        "sample_tokens[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af51439b",
      "metadata": {
        "id": "af51439b"
      },
      "outputs": [],
      "source": [
        "sample_tokens[\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587aa9a4",
      "metadata": {
        "id": "587aa9a4"
      },
      "outputs": [],
      "source": [
        "tokens = [tokenizer(i, padding=\"max_length\", max_length=seq_len,\n",
        "                    truncation=True, return_tensors=\"pt\")\n",
        "         for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1271b5c2",
      "metadata": {
        "id": "1271b5c2"
      },
      "source": [
        "### Save the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b51be73",
      "metadata": {
        "id": "4b51be73"
      },
      "outputs": [],
      "source": [
        "save_file(tokens_path, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323823d5",
      "metadata": {
        "id": "323823d5"
      },
      "source": [
        "## Create Bert model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4543e94",
      "metadata": {
        "id": "e4543e94"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        for param in self.bert.parameters():\n",
        "            param.required_grad = False\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, num_classes)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, bert_output = self.bert(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  return_dict=False)\n",
        "        dropout_output = self.activation(self.dropout(bert_output))\n",
        "        final_output = self.linear(dropout_output)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b32327e",
      "metadata": {
        "id": "5b32327e"
      },
      "source": [
        "## Create PyTorch Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2969357",
      "metadata": {
        "id": "c2969357"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.tokens = tokens\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.tokens[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d140aa21",
      "metadata": {
        "id": "d140aa21"
      },
      "source": [
        "### Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6deedcb3",
      "metadata": {
        "id": "6deedcb3"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "          device, num_epochs, model_path):\n",
        "    \"\"\"\n",
        "    Function to train the model\n",
        "    :param train_loader: Data loader for train dataset\n",
        "    :param valid_loader: Data loader for validation dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param optimizer: Optimizer\n",
        "    :param device: CUDA or CPU\n",
        "    :param num_epochs: Number of epochs\n",
        "    :param model_path: Path to save the model\n",
        "    \"\"\"\n",
        "    best_loss = 1e8\n",
        "    for i in range(num_epochs):\n",
        "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
        "        valid_loss, train_loss = [], []\n",
        "        model.train()\n",
        "        # Train loop\n",
        "        for batch_labels, batch_data in tqdm(train_loader):\n",
        "            input_ids = batch_data[\"input_ids\"]\n",
        "            attention_mask = batch_data[\"attention_mask\"]\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            input_ids = torch.squeeze(input_ids, 1)\n",
        "            # Forward pass\n",
        "            batch_output = model(input_ids, attention_mask)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            batch_labels = batch_labels.type(torch.LongTensor)\n",
        "            batch_output = batch_output.type(torch.FloatTensor)\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            train_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Gradient update step\n",
        "            optimizer.step()\n",
        "        model.eval()\n",
        "        # Validation loop\n",
        "        for batch_labels, batch_data in tqdm(valid_loader):\n",
        "            input_ids = batch_data[\"input_ids\"]\n",
        "            attention_mask = batch_data[\"attention_mask\"]\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            input_ids = torch.squeeze(input_ids, 1)\n",
        "            # Forward pass\n",
        "            batch_output = model(input_ids, attention_mask)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            batch_labels = batch_labels.type(torch.LongTensor)\n",
        "            batch_output = batch_output.type(torch.FloatTensor)\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            valid_loss.append(loss.item())\n",
        "        t_loss = np.mean(train_loss)\n",
        "        v_loss = np.mean(valid_loss)\n",
        "        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n",
        "        if v_loss < best_loss:\n",
        "            best_loss = v_loss\n",
        "            # Save model if validation loss improves\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Best Validation Loss: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc7f7da",
      "metadata": {
        "id": "2fc7f7da"
      },
      "source": [
        "### Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be319b",
      "metadata": {
        "id": "19be319b"
      },
      "outputs": [],
      "source": [
        "def test(test_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Function to test the model\n",
        "    :param test_loader: Data loader for test dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param device: CUDA or CPU\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    test_accu = []\n",
        "    for batch_labels, batch_data in tqdm(test_loader):\n",
        "        input_ids = batch_data[\"input_ids\"]\n",
        "        attention_mask = batch_data[\"attention_mask\"]\n",
        "        # Move data to GPU if available\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        input_ids = torch.squeeze(input_ids, 1)\n",
        "        # Forward pass\n",
        "        batch_output = model(input_ids, attention_mask)\n",
        "        batch_output = torch.squeeze(batch_output)\n",
        "        # Calculate loss\n",
        "        batch_labels = batch_labels.type(torch.LongTensor)\n",
        "        batch_output = batch_output.type(torch.FloatTensor)\n",
        "        loss = criterion(batch_output, batch_labels)\n",
        "        test_loss.append(loss.item())\n",
        "        batch_preds = torch.argmax(batch_output, axis=1)\n",
        "        # Move predictions to CPU\n",
        "        if torch.cuda.is_available():\n",
        "            batch_labels = batch_labels.cpu()\n",
        "            batch_preds = batch_preds.cpu()\n",
        "        # Compute accuracy\n",
        "        test_accu.append(accuracy_score(batch_labels.detach().\n",
        "                                        numpy(),\n",
        "                                        batch_preds.detach().\n",
        "                                        numpy()))\n",
        "    test_loss = np.mean(test_loss)\n",
        "    test_accu = np.mean(test_accu)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e896845",
      "metadata": {
        "id": "9e896845"
      },
      "source": [
        "## Train Bert model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d98212",
      "metadata": {
        "id": "b3d98212"
      },
      "source": [
        "### Load the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321f7a2f",
      "metadata": {
        "id": "321f7a2f"
      },
      "outputs": [],
      "source": [
        "tokens = load_file(tokens_path)\n",
        "labels = load_file(labels_path)\n",
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1619263d",
      "metadata": {
        "id": "1619263d"
      },
      "source": [
        "### Split data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6178ebb",
      "metadata": {
        "id": "e6178ebb"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tokens, labels,\n",
        "                                                   test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
        "                                                      y_train,\n",
        "                                                     test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae6c092",
      "metadata": {
        "id": "2ae6c092"
      },
      "source": [
        "### Create PyTorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2202470e",
      "metadata": {
        "id": "2202470e"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(X_train, y_train)\n",
        "valid_dataset = TextDataset(X_valid, y_valid)\n",
        "test_dataset = TextDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe3d735",
      "metadata": {
        "id": "bfe3d735"
      },
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d03cf88",
      "metadata": {
        "id": "0d03cf88"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                         batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c882cf2",
      "metadata": {
        "id": "2c882cf2"
      },
      "source": [
        "### Create model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1c92f1",
      "metadata": {
        "id": "8e1c92f1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                     else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6053ecaf",
      "metadata": {
        "id": "6053ecaf"
      },
      "outputs": [],
      "source": [
        "model = BertClassifier(dropout, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ecf048",
      "metadata": {
        "id": "24ecf048"
      },
      "source": [
        "### Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794e3c8f",
      "metadata": {
        "id": "794e3c8f"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1665a786",
      "metadata": {
        "id": "1665a786"
      },
      "source": [
        "### Move the model to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c0dfbc",
      "metadata": {
        "id": "19c0dfbc"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d331f9f8",
      "metadata": {
        "id": "d331f9f8"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffb8343",
      "metadata": {
        "id": "8ffb8343"
      },
      "outputs": [],
      "source": [
        "train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "     device, num_epochs, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "426c4238",
      "metadata": {
        "id": "426c4238"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9a2cd8",
      "metadata": {
        "id": "1c9a2cd8"
      },
      "outputs": [],
      "source": [
        "test(test_loader, model, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbb7a07",
      "metadata": {
        "id": "dbbb7a07"
      },
      "source": [
        "## Predict on new text\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e7b40a",
      "metadata": {
        "id": "c5e7b40a"
      },
      "outputs": [],
      "source": [
        "input_text = '''I am a victim of Identity Theft & currently have an Experian account that\n",
        "I can view my Experian Credit Report and getting notified when there is activity on\n",
        "my Experian Credit Report. For the past 3 days I've spent a total of approximately 9\n",
        "hours on the phone with Experian. Every time I call I get transferred repeatedly and\n",
        "then my last transfer and automated message states to press 1 and leave a message and\n",
        "someone would call me. Every time I press 1 I get an automatic message stating than you\n",
        "before I even leave a message and get disconnected. I call Experian again, explain what\n",
        "is happening and the process begins again with the same end result. I was trying to have\n",
        "this issue attended and resolved informally but I give up after 9 hours. There are hard\n",
        "hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall\n",
        "and I respectfully request that Experian remove the hard hit inquiries immediately just\n",
        "like they've done in the past when I was able to speak to a live Experian representative\n",
        "in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX\n",
        "XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX\n",
        "XX/XX/XXXX'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a65bba8",
      "metadata": {
        "id": "1a65bba8"
      },
      "outputs": [],
      "source": [
        "input_text = input_text.lower()\n",
        "input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n",
        "input_text = re.sub(\"\\d+\", \"\", input_text)\n",
        "input_text = re.sub(r'[x]{2,}', \"\", input_text)\n",
        "input_text = re.sub(' +', ' ', input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e724e3e",
      "metadata": {
        "id": "5e724e3e"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868f2a4b",
      "metadata": {
        "id": "868f2a4b"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(input_text, padding=\"max_length\",\n",
        "                 max_length=seq_len, truncation=True,\n",
        "                 return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b0bd40",
      "metadata": {
        "id": "62b0bd40"
      },
      "outputs": [],
      "source": [
        "input_ids = tokens[\"input_ids\"]\n",
        "attention_mask = tokens[\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565f8ae8",
      "metadata": {
        "id": "565f8ae8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                     else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1b7751",
      "metadata": {
        "id": "1d1b7751"
      },
      "outputs": [],
      "source": [
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fb78ad",
      "metadata": {
        "id": "82fb78ad"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.squeeze(input_ids, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87b4155",
      "metadata": {
        "id": "d87b4155"
      },
      "outputs": [],
      "source": [
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9721e08",
      "metadata": {
        "id": "b9721e08"
      },
      "outputs": [],
      "source": [
        "# Create model object\n",
        "model = BertClassifier(dropout, num_classes)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Forward pass\n",
        "out = torch.squeeze(model(input_ids, attention_mask))\n",
        "\n",
        "# Find predicted class\n",
        "prediction = label_encoder.classes_[torch.argmax(out)]\n",
        "print(f\"Predicted Class: {prediction}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}